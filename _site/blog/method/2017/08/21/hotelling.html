<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Hotelling&#39;s Method</title>
  <meta name="description" content="I would like to use this first blog post to introduce Hotelling’s method for computing the inverse of a matrix. We’ll focus on the symmetric case. First let’...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/method/2017/08/21/hotelling.html">
  <link rel="alternate" type="application/rss+xml" title="William Dawson Github" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">William Dawson Github</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/current/">Current Projects</a>
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Hotelling&#39;s Method</h1>
    <p class="post-meta">
      <time datetime="2017-08-21T11:00:00+09:00" itemprop="datePublished">
        
        Aug 21, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>I would like to use this first blog post to introduce Hotelling’s method
for computing the inverse of a matrix. We’ll focus on the symmetric case.
First let’s begin by asking why
we would want to directly compute the inverse of a matrix. After all,
one of the first things you’ll learn in any numerical linear algebra class
is to always avoid explicitly inverting a matrix.</p>

<p>However, in quantum chemistry it’s actually fairly normal to compute
a matrix inverse. In particular, when solving the generalized eigenvalue
problem:</p>

<script type="math/tex; mode=display">\begin{equation}H\phi = \lambda S \phi.\end{equation}</script>

<p>We can reduce this generalized eigenvalue problem to the standard eigenvalue
problem using the inverse square root of the overlap matrix:</p>

<script type="math/tex; mode=display">\begin{equation}
S^{-\frac{1}{2}}HS^{-\frac{1}{2}}\phi = \lambda \phi.
\end{equation}</script>

<p>Performing this calculation is acceptable because we can reuse the inverse
over many scf loops, and because the overlap matrix is usually well conditioned.
In the canonical density matrix purification method of Palser[1] (which
we will probably discuss in more detail later), one also finds the matrix
inverse:</p>

<script type="math/tex; mode=display">\begin{equation}
p_0 = \frac{\lambda}{2}(\mu S^{-1} - S^{-1}HS^{-1}) + \frac{1}{2}S^{-1}
\end{equation}</script>

<p>where <script type="math/tex">p_0</script> is an initial guess at the density matrix.</p>

<p>Hotelling’s method is very simple to implement. It works through the
following iteration:</p>

<script type="math/tex; mode=display">\begin{equation}
X_{n+1} = 2X_{n} - X_{n}SX_{n}
\end{equation}</script>

<p>where <script type="math/tex">\lim_{n \to \infty} X_n = S^{-1}</script>. Palser cites the famous Numerical
Recipes book for this method. Disappointingly, it seems that this method
is called “Hotelling’s Method” because it was invented by statistician Harold
Hotelling, and not because it leaves chocolates on your pillows. I’ve seen some
people mention reference [2] as the original paper. Hotelling himself notes that
it was “noticed” in reference [3].</p>

<p>The condition for convergence is (from [2]) <script type="math/tex">% <![CDATA[
\|1 - SX_0 \| < 1 %]]></script>. So
how should we pick an initial <script type="math/tex">X_0</script>? One simple way is to just scale the
initial matrix. Consider the eigendecomposition of our initial matrix:</p>

<script type="math/tex; mode=display">\begin{equation}
S = UDU^T.
\end{equation}</script>

<p>Now let’s plug the decomposition into the convergence condition, with a
scaling value <script type="math/tex">\alpha</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\|1 - UDU^T\alpha UDU^T \| < 1.
\end{equation} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\|1 - \alpha DD \| < 1.
\end{equation} %]]></script>

<p>The effect of multiplying the diagonal matrix of eigenvalues is to square all
the eigenvalues. Hence they are all positive. Thus is <script type="math/tex">\alpha</script> is equal
to the inverse of the largest eigenvalue squared, we satisfy the equation.
The largest eigenvalue can be cheaply computed using the power method.</p>

<p>This is just a simple starting guess that I’ve introduced, there are better ones
out there in the literature, such as the guess in reference [4] which is
specific to overlap matrices (homework question: how can we improve our
<script type="math/tex">\alpha</script> value for overlap matrices?).</p>

<p>Let’s take a look at a simple implementation:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Libraries</span>
<span class="kn">from</span> <span class="nn">scipy.sparse.linalg</span> <span class="kn">import</span> <span class="n">eigsh</span><span class="p">,</span><span class="n">norm</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">identity</span><span class="p">,</span> <span class="n">rand</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">argv</span>

<span class="c"># Input Parameters</span>
<span class="n">dimension</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">sparsity</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="c"># Initial Matrix</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span> <span class="n">dimension</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="n">sparsity</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s">"csr"</span><span class="p">)</span>
<span class="n">matrix</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">matrix</span><span class="o">.</span><span class="n">T</span>

<span class="c"># Initial Guess</span>
<span class="n">largest_eigenvalue</span> <span class="o">=</span> <span class="n">eigsh</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">'LM'</span><span class="p">,</span>
  <span class="n">return_eigenvectors</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">inverse_mat</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">*</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">largest_eigenvalue</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c"># Iterate</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">inverse_mat</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">inverse_mat</span> <span class="o">-</span> \
    <span class="n">inverse_mat</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">matrix</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inverse_mat</span><span class="p">))</span>
  <span class="n">norm_value</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">inverse_mat</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="n">identity</span><span class="p">(</span><span class="n">dimension</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">norm_value</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">:</span>
    <span class="k">break</span>

<span class="k">print</span> <span class="n">norm</span><span class="p">(</span><span class="n">inverse_mat</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="n">identity</span><span class="p">(</span><span class="n">dimension</span><span class="p">))</span></code></pre></figure>

<p>The implementation highlights two further features of this algorithm. First,
the main computational kernel is matrix multiplication, which is great for
high performance computing. Second, it’s trivial to extend to the sparse case
by simply replacing the dense multiplies with sparse multiplies.</p>

<blockquote>
  <p>[1] Palser, Adam HR, and David E. Manolopoulos. “Canonical purification of the
density matrix in electronic-structure theory.” Physical Review B 58, no. 19
(1998): 12704.</p>
</blockquote>

<blockquote>
  <p>[2] Hotelling, Harold. “Some new methods in matrix calculation.” The Annals
of Mathematical Statistics 14, no. 1 (1943): 1-34.</p>
</blockquote>

<blockquote>
  <p>[3] Frazer, Robert Alexander, William Jolly Duncan, Arthur Roderich Collar,
and A. A. Mullin. “Elementary matrices and some applications to dynamics and
differential equations.” American Journal of Physics 29, no. 8 (1961): 555-556.</p>
</blockquote>

<blockquote>
  <p>[4] Ozaki, T. “Efficient recursion method for inverting an overlap matrix.”
Physical Review B 64, no. 19 (2001): 195110.</p>
</blockquote>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">William Dawson Github</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              William Dawson Github
            
            </li>
            
            <li><a href="mailto:william.dawson@riken.jp">william.dawson@riken.jp</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/william-dawson"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">william-dawson</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>An overview of my projects on github, and updates on my current work.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
